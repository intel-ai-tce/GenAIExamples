# Copyright (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

vllm:
  image:
    repository: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo
    tag: "v0.9.2"
  resources: {}
  LLM_MODEL_ID: meta-llama/Meta-Llama-3-8B-Instruct
  # Uncomment the following model specific settings for DeepSeek models
  VLLM_CPU_KVCACHE_SPACE: 40

  extraCmdArgs: [
    "--tensor-parallel-size", "2",
    "--block-size", "128",
    "--dtype", "bfloat16",
    "--max-model-len","5196",
    "--distributed_executor_backend", "mp",
    "--enable_chunked_prefill",
    "--enforce-eager"]
  #resources:
  #  requests:
  #    memory: 60Gi # 40G for KV cache, and 20G for DeepSeek-R1-Distill-Qwen-7B, need to adjust it for other models
